{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"datasetVersion","sourceId":2431805,"datasetId":8782,"databundleVersionId":2474046}],"dockerImageVersionId":31287,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nfrom pathlib import Path\nimport cv2\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom albumentations import ToTensorV2\nimport albumentations as A\nimport random\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.optim import Adam\nfrom torchmetrics.classification import MulticlassF1Score, MulticlassAccuracy,MulticlassRecall\n\n\ndef seed_everything(seed: int = 42):\n    random.seed(seed)          # python random\n    np.random.seed(seed)       # numpy\n    torch.manual_seed(seed)    # torch CPU\n    torch.cuda.manual_seed_all(seed)\n\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nseed_everything(42)\n\nbase_path=Path(r\"/kaggle/input/datasets/alxmamaev/flowers-recognition/flowers\")\npath_list=[]\n\nfor img_path in base_path.rglob(\"*.jpg\"):\n    path_list.append({\"label\":img_path.parent.name,\"path\":img_path})\n\ndf=pd.DataFrame(path_list)\ndf['targets']=pd.factorize(df['label'])[0]\ndf=df.sample(frac=1,random_state=42).reset_index(drop=True)\n\ntrain_df,tmp_df=train_test_split(df,test_size=0.3,stratify=df['label'],random_state=42)\nval_df,test_df=train_test_split(tmp_df,test_size=0.4,stratify=tmp_df['label'],random_state=42)\n\nimg_aug=A.Compose([\n    A.RandomResizedCrop(size=(224,224),scale=(0.8,1.0),ratio=(0.9,1.1),p=1),\n    A.HorizontalFlip(p=0.3),\n    A.Affine(scale=(0.9,1.1),rotate=(-15,15),border_mode=cv2.BORDER_REFLECT_101,p=0.3),\n    A.ColorJitter(brightness=0.2,contrast=0.2,saturation=0.2,hue=0.03,p=0.6),\n    A.CoarseDropout(num_holes_range=(1, 1), hole_height_range=(48, 48),\n                    hole_width_range=(48, 48),fill=0,p=0.25)\n])\n\ntr_resnet34=A.Compose([\n    A.RandomResizedCrop(size=(224,224),scale=(0.8,1.0),ratio=(0.9,1.1),p=1),\n    A.HorizontalFlip(p=0.3),\n    A.Affine(scale=(0.9,1.1),rotate=(-15,15),border_mode=cv2.BORDER_REFLECT_101,p=0.3),\n    A.ColorJitter(brightness=0.2,contrast=0.2,saturation=0.2,hue=0.03,p=0.6),\n    A.CoarseDropout(num_holes_range=(1, 1), hole_height_range=(48, 48),\n                    hole_width_range=(48, 48),fill=0,p=0.25),\n    A.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)),\n    ToTensorV2()\n])\n\nval_resnet34=A.Compose([\n    A.Resize(224,224,p=1),\n    A.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)),\n    ToTensorV2()\n])\n\nclass FlowerCustom(Dataset):\n    def __init__(self,path,targets,augment=None):\n        self.path=path\n        self.targets=targets\n        self.augment=augment\n    def __len__(self):\n        return len(self.path)\n    def __getitem__(self,idx):\n        img=cv2.imread(self.path[idx])\n        img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n        if self.augment is None:\n            raise ValueError(\"IMG Augment must be need\")\n        img=self.augment(image=img)['image']\n        targets=torch.tensor(self.targets[idx],dtype=torch.long)\n        return img,targets\n\n\ntrain_custom=FlowerCustom(train_df['path'].to_list(),train_df['targets'].to_list(),\n                          augment=tr_resnet34)\nval_custom=FlowerCustom(val_df['path'].to_list(),val_df['targets'].to_list(),\n                        augment=val_resnet34)\ntest_custom=FlowerCustom(test_df['path'].to_list(),test_df['targets'].to_list(),\n                         augment=val_resnet34)\n\ntrain_loader=DataLoader(train_custom,batch_size=32,shuffle=True,num_workers=4,pin_memory=True)\nval_loader=DataLoader(val_custom,batch_size=32,shuffle=False,num_workers=4,pin_memory=True)\ntest_loader=DataLoader(test_custom,batch_size=32,shuffle=False,num_workers=4,pin_memory=True)\n\nprint(\"done\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"k,v=next(iter(train_loader))\nk.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SimpleCNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_layer=nn.Sequential(\n            nn.Conv2d(3,64,kernel_size=3,stride=1,padding=1),\n            nn.LeakyReLU(0.1),\n            nn.BatchNorm2d(64),\n            nn.MaxPool2d(2),\n\n            nn.Conv2d(64,128,kernel_size=3,stride=1,padding=1),\n            nn.LeakyReLU(0.1),\n            nn.BatchNorm2d(128),\n            nn.MaxPool2d(2),\n\n            nn.Conv2d(128,256,kernel_size=3,stride=1,padding=1),\n            nn.LeakyReLU(0.1),\n            nn.BatchNorm2d(256),\n            nn.MaxPool2d(2),\n            nn.AdaptiveAvgPool2d((1,1)) \n        )\n\n        self.fc_layer=nn.Sequential(\n            nn.Linear(256,128),\n            nn.LeakyReLU(0.1),\n            nn.BatchNorm1d(128),\n            nn.Linear(128,5),\n            nn.LogSoftmax(dim=-1)\n        )\n\n    def forward(self,x):\n        x=self.conv_layer(x)\n        x=torch.flatten(x,1)\n        x=self.fc_layer(x)\n        return x\n\n    def forward_logit(self,model,x): # logit값만 구하는 로\n        feats=model.conv_layer(x)\n        feats=torch.flatten(feats,1) # 0번 차원(B)은 그대로 두고, 1번 차원부터 끝까지 flatten하라는것. (B,1152)\n        logits=model.fc_layer[:-1](feats) # 시퀀셜 인덱싱. LogSoftmax전까지만 슬라이싱\n        return logits\n\nfrom typing import List\nfrom dataclasses import dataclass,field\nfrom tqdm import tqdm\n\n@dataclass\nclass History:\n    training_accuracy:List[float]=field(default_factory=list)\n    training_recall:List[float]=field(default_factory=list)\n    training_loss:List[float]=field(default_factory=list)\n    val_accuracy:List[float]=field(default_factory=list)\n    val_recall:List[float]=field(default_factory=list)\n    val_loss:List[float]=field(default_factory=list)\nhistory=History()\n\n\nclass Trainer:\n    def __init__(self,train_loader,val_loader,model,optimizer,loss_func,\n                 scheduler,metric_acc,metric_rec,device,history,mode=\"min\"):\n        self.model=model\n        self.train_loader=train_loader\n        self.val_loader=val_loader\n        self.optimizer=optimizer\n        self.loss_func=loss_func\n        self.scheduler=scheduler\n        self.metric_acc=metric_acc\n        self.metric_rec=metric_rec\n        self.device=device\n        self.history=history\n        if mode==\"max\":\n            self.best_value=float('-inf')\n        else:\n            self.best_value=float('inf')\n\n    def training_epoch(self,epoch):\n        self.metric_acc.reset()\n        self.metric_rec.reset()\n        self.model.train()\n        loss_sum=0.0\n        avg_loss=0.0\n        with tqdm(total=len(self.train_loader),desc=f\"training {epoch}\",leave=True) as bar:\n            for batch_idx,(x_train,y_train) in enumerate(self.train_loader):\n                x_train=x_train.to(self.device)\n                y_train=y_train.to(self.device)\n                logits=self.model(x_train)\n                loss=self.loss_func(logits,y_train)\n                self.optimizer.zero_grad()\n                loss.backward()\n                self.optimizer.step()\n                loss_sum+=loss.item()\n                avg_loss=loss_sum/(batch_idx+1)\n                preds=logits.argmax(dim=1)   # dim=-1과 같다. (B,12)  1번 dim 즉 행에 대해서\n                self.metric_acc.update(preds, y_train)\n                self.metric_rec.update(preds, y_train)\n                bar.update(1)\n\n                if batch_idx%10==0:\n                    acc=self.metric_acc.compute().item()\n                    recall=self.metric_rec.compute().item()\n                    bar.set_postfix({\"acc\": acc, \"recall\":recall, \"loss\":avg_loss,\"epoch\":epoch})\n            return self.metric_acc.compute().item(), self.metric_rec.compute().item(),avg_loss  \n\n    def validating_epoch(self,epoch):\n        self.metric_acc.reset()\n        self.metric_rec.reset()\n        self.model.eval()\n        loss_sum=0\n        avg_loss=0.0\n        with tqdm(total=len(self.val_loader),desc=f\"validating {epoch}\", leave=True) as bar:\n            with torch.no_grad():\n                for batch_idx,(x_val,y_val) in enumerate(self.val_loader):\n                    x_val=x_val.to(self.device)\n                    y_val=y_val.to(self.device)\n                    logits=self.model(x_val)\n                    loss=self.loss_func(logits,y_val)\n\n                    preds=logits.argmax(dim=-1)\n                    self.metric_acc.update(preds,y_val)\n                    self.metric_rec.update(preds,y_val)\n                    loss_sum+=loss.item()\n                    avg_loss=loss_sum/(batch_idx+1)\n                    bar.update(1)\n                    if batch_idx%10==0:\n                        acc=self.metric_acc.compute().item()\n                        recall=self.metric_rec.compute().item()\n                        bar.set_postfix({\"acc\": acc, \"recall\":recall, \"loss\":avg_loss,\"epoch\":epoch})\n                return self.metric_acc.compute().item(), self.metric_rec.compute().item(),avg_loss\n\n    \n    def fit(self,epochs,early_stop,path):\n        stop_count=0   \n        for epoch in range(epochs):\n            training_accuracy,training_recall,training_loss=self.training_epoch(epoch)\n            self.history.training_accuracy.append(training_accuracy)\n            self.history.training_recall.append(training_recall)\n            self.history.training_loss.append(training_loss)\n            val_accuracy,val_recall,val_loss=self.validating_epoch(epoch)\n            self.history.val_accuracy.append(val_accuracy)\n            self.history.val_recall.append(val_recall)\n            self.history.val_loss.append(val_loss)\n            \n            if self.best_value>val_loss:\n                self.best_value=val_loss\n                stop_count=0\n                torch.save(self.model.state_dict(),os.path.join(path,f\"{epoch}_{val_loss}.pt\"))\n            else:\n                stop_count+=1\n                if stop_count>=early_stop:\n                    print(f\"early_stopped. current epoch : {epoch}\")\n                    return self.history\n                    \n        return self.history\n\nfrom torch.optim import Adam\nfrom torchmetrics import Accuracy,Recall\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport os\n\ndevice=\"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel=SimpleCNN()\nmodel=model.to(device)\noptimizer=Adam(model.parameters(),lr=1e-3)\nloss_func = nn.NLLLoss()\nacc_metric=Accuracy(task=\"multiclass\",num_classes=5)\nrecall_metric=Recall(task=\"multiclass\",num_classes=5,average=\"macro\")\nscheduler=ReduceLROnPlateau(optimizer,factor=0.1,patience=3)\nmodel=model.to(device)\nmetric_rec=recall_metric.to(device)\nmetric_acc=acc_metric.to(device)\n\noutput_path=r\"/kaggle/working/\"\nt=Trainer(train_loader,val_loader,model,optimizer,loss_func,scheduler,acc_metric,recall_metric,device,history,mode=\"min\")\nhistory=t.fit(30,5,output_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\nbest_param=torch.load(r\"/kaggle/working/8_0.8227456068992615.pt\")\nmodel.load_state_dict(best_param)\n\nclass Predict:\n    def __init__(self,model,test_loader,device):\n        self.model=model.to(device)\n        self.test_loader=test_loader\n        self.device=device\n        self.actual_list=[]\n        self.pred_list=[]\n    def predict(self):\n        with tqdm(total=len(self.test_loader),desc=f\"predicting\",leave=True) as bar:\n            self.model.eval()\n            for x,y in self.test_loader:\n                x=x.to(self.device)\n                y=y.to(self.device)\n                logits=self.model(x)\n                pred=torch.argmax(logits,dim=-1)\n                self.pred_list.extend(pred.detach().cpu().numpy().tolist())\n                self.actual_list.extend(y.detach().cpu().numpy().tolist())\n                bar.update(1)\n            return self.actual_list,self.pred_list\n\np=Predict(model,test_loader,device)\nanswer,preds=p.predict()\ncm=confusion_matrix(answer,preds)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\nclass GradCAM:\n    def __init__(self, model, target_layer_idx=10 ):\n        self.model = model\n        self.target_layer = model.conv_layer[target_layer_idx] # CAM을 뽑을 레어어선택(주로 Conv출력선)\n\n        self.feature_maps = None # hook으로 잡아둘 데이터공간(target layer의 출력(feature map))\n        self.gradients = None   # hook으로 잡아둘 데이터공간(backward 때 그 출력에 대한 gradient)\n        # None으로 설정한 이유는 아직 값이 없다는것을 명확히 하기위해. \n        # 아직 forward/backward를 안 거쳤다\"는 의미의 초기 상태 표시용\n\n        self.fwd_handle = self.target_layer.register_forward_hook(self._forward_hook) \n        # register_forward_hook은 nn.Module의 공식 API로 레이어 forward가 끝난 직후 자동 실행되는 콜백 등록 함수\n        # 인자로 hook_fn을 받는다(여기선 아래의 _forward_hook => (module, input, output)을 받는 hook_fn을 받음\n        \n        self.bwd_handle = self.target_layer.register_full_backward_hook(self._backward_hook)\n        # full_backward_hook 권장 (PyTorch 최신)-(module, input, output) 을 받는 함수를 인자로 받는다\n        # 위와 통일\n\n    def _forward_hook(self, module, inputs, output):\n        # hook_fn으로 module은 타겟레이어, inputs는 (x,) <=항상 튜플로 받아야함!\n        # output은 feature map : (B, C, H, W)\n        # module, input, output은 PyTorch가 자동으로 넣어준다\n        self.feature_maps = output\n\n    def _backward_hook(self, module, grad_input, grad_output):\n        # grad_output[0]: (B, C, H, W)\n        self.gradients = grad_output[0]\n        # Conv레이어는 입력이 1개, 출력이 1개. grad_output = (tensor,) 이런 구조\n        # [0]으로 인덱싱해서 tuple 안에 들어있는 진짜 gradient 텐서만 꺼내는 것\n        # 만약 인덱싱을 안하면 텐서가 아닌 튜플값이 들어와 연산이 안됨\n        # 직관적으로 이해하긴 쉽지 않지만 메서드내에서 직접 안쓰이는데 인자들이 있는것은 pytorch가 정해준 형식이라 어쩔수없음.\n\n    def remove_hooks(self):\n        self.fwd_handle.remove()\n        self.bwd_handle.remove()\n\n    @torch.no_grad() # 계산그래프를 만들지마라는 데코레이터. 그래서 나중에 .backward()를 해도 이 연산은 추적대상이 아니다\n    def _normalize(self, cam): # CAM 값을 0~1 범위로 정규화하는 코드\n        # Grad-CAM은 히트맵이니까 0~1로 맞춰야 컬러맵 적용이 자연스럽게 적용\n        # 다른 정규화보다 굳이 아래와 같이 하는 이유는 Grad-CAM은 “시각화용 상대 스케일”이 목적이라 min-max가 가장 적\n        \n        cam = cam - cam.min() # 텐서의 모든 원소에 대해 최소값을 빼는 연산. 그러면 -(-최소값)하면 최소값은 0이 된다\n        cam = cam / (cam.max() + 1e-8) # 최대값으로 나눠 0~1사이로 정규화. 행여 max가 0이면 에러방지용으로 +1e-8\n        return cam\n\n    def generate(self, x, class_idx=None):\n        \"\"\"\n        x: (1,1,28,28) tensor on device\n        class_idx: None이면 예측 클래스로 CAM 생성\n        return: cam (H,W) in [0,1], pred_idx, score\n        \"\"\"\n        self.model.eval() # train모드에서는 dropout이 활성화. Grad-CAM 결정된 예측에 대해 어디를 봤는지 설명하는 것\n        self.model.zero_grad(set_to_none=True)\n\n        # forward\n        out = self.model(x)  # (1,10)  현재는 log-prob\n        # 학습을 다시 하는 게 아니라, 이 입력 x에 대한 forward 결과와 gradient를 얻기 위해 다시 모델에 넣는것\n        # 학습은 많은 데이터에 대해 가중치를 최적화하는 과정. Grad-CAM은 특정 입력 한 장에 대한 gradient 분석\n        \n        pred_idx = int(out.argmax(dim=-1).item()) # 모델이 예측한 클래스번\n\n        if class_idx is None:\n            class_idx = pred_idx\n\n        score = out[0, class_idx]   # 스칼라 (log-prob)\n        # 텐서인덱싱. 첫번째 배치의 class_idx번째 클래스. 스칼라 값\n\n        # backward: score에 대한 grad\n        score.backward(retain_graph=False)\n        # 특정 클래스 점수가 증가하려면, feature map의 어떤 채널이 얼마나 중요했는지 알고 싶기에\n        # 이 클래스 점수에 영향을 준 feature map의 중요도(gradient)를 얻기 위한 단계\n\n        \n        # gradients: (1,C,H,W), feature_maps : (1,C,H,W)\n        grads = self.gradients  # 계산 편의성과 가독성을 위해, self.에서 로컬 변수로 빼서 쓰는 것\n        acts = self.feature_maps\n\n\n        ### print(\"acts None?\", acts is None, \"grads None?\", grads is None)\n        ### print(\"acts min/max/mean:\", acts.min().item(), acts.max().item(), acts.mean().item())\n        #### print(\"grads min/max/mean:\", grads.min().item(), grads.max().item(), grads.mean().item())\n\n\n        # channel-wise weight: GAP over (H,W)\n        # 핵심구현 부분 \n        # grads.shape는 (1,C,H,W). dim=(2,3)은 H,W. 각 채널 안에서 공간 전체 평균을 내는 것\n        # (1,C)로 변환. 채널 하나당 평균값 1개로 요약됨\n        # torch의 mean은 기본적으로 평균낸 차원을 없앤다. \n        # 나중에 곱셈을 해야 하니까 차원을 그대로 유지하기 위해 keepdim=True\n        weights = grads.mean(dim=(2,3), keepdim=True)  # (1,C,1,1)\n\n\n        # weights.shape = (1, C, 1, 1)\n        # acts.shape(feature_map) = (1, C, H, W)\n        # weights * acts => broadcating으로 (1,C,H,W)\n        # 각 채널의 전체 feature map에 그 채널 weight를 곱하는 것\n        # .sum => 해당 차원(dim=1, 즉 채널차원)을 모두 더해서 하나의 지도 생성\n        # keepdim=True로 채널차원 1로 유지. torch의 sum도 해당차원이 사라지나 keepdim해서 그값을 거기에 크기 1짜리 차원으로 유지\n        cam = (weights * acts).sum(dim=1, keepdim=True)  # (1,1,H,W)\n\n        # ReLU - 음수제거하기 위해. relu를 안하면 heatmap에 음수 영역도 포함됨\n        # 시각적으로 덜 직관적. Grad-CAM은 설명을 단순화하기 위해 positive evidence만 남기는 것\n      ###   print(\"pre-ReLU cam min/max/mean:\",\n      ### cam.min().item(), cam.max().item(), cam.mean().item())\n      ###   cam = F.relu(cam)\n\n        # input size로 업샘플\n        # Grad-CAM을 이미지 위에 덮어씌우려면 입력이미지와 같은 크기여야 한다\n        # target layer의 feature map 크기는 Conv와 Pool을 거치면 7x7,14x14 등등이 될수있다\n        # mode=\"bilinear\" 업샘플링 방식지정. 부드럽게 보간\n        # align_corners=False. bilinear 보간 시 좌표 정렬 방식 옵션.False → 일반적으로 더 안정적\n        cam = F.interpolate(cam, size=(x.shape[2], x.shape[3]), mode=\"bilinear\", align_corners=False)\n\n        # interpolate 이후 cam.shape = (1, 1, H, W)\n        # cam[0,0] => 첫 번째 batch의 첫번째 채널선택 => (H,W)\n        # 배치가 1이고 채널도 1이므로, cam[0,0]은 단순히 차원 정리용 선택\n        cam = cam[0,0].detach()\n        cam = self._normalize(cam)\n\n        return cam.cpu(), pred_idx, float(score.detach().cpu().item())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"images, labels = next(iter(test_loader))      # images: (B,1,28,28)\nx = images[0:1].to(device)                    # (1,1,28,28)\ny = labels[0].item()\n\ngradcam = GradCAM(model, target_layer_idx=8)\n\ncam, pred_idx, score = gradcam.generate(x)  # cam: (H,W) on cpu\ngradcam.remove_hooks()\nprint(\"done\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# 원본 이미지 (cpu로)\nimg = x[0, 0].detach().cpu()   # (28,28)\n\nplt.figure(figsize=(10,3))\n\n# 1) 원본\nplt.subplot(1,3,1)\nplt.title(f\"Input (label={y})\")\nplt.imshow(img, cmap=\"gray\")\nplt.axis(\"off\")\n\n# 2) CAM 히트맵\nplt.subplot(1,3,2)\nplt.title(f\"Grad-CAM (pred={pred_idx})\")\nplt.imshow(cam, cmap=\"jet\")\nplt.axis(\"off\")\n\n# 3) 오버레이\nplt.subplot(1,3,3)\nplt.title(f\"Overlay (score={score:.3f})\")\nplt.imshow(img, cmap=\"gray\")\nplt.imshow(cam, cmap=\"jet\", alpha=0.45)  # alpha로 투명도 조절\nplt.axis(\"off\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport torch\n\ndef to_numpy_img(x: torch.Tensor):\n    \"\"\"\n    x: (1,1,H,W) or (1,H,W) or (H,W)\n    return: (H,W) float in [0,1]\n    \"\"\"\n    if x.ndim == 4:\n        x = x[0, 0]\n    elif x.ndim == 3:\n        x = x[0]\n    x = x.detach().float().cpu()\n    x = x - x.min()\n    x = x / (x.max() + 1e-8)\n    return x.numpy()\n\ndef overlay_cam_on_grayscale(img_hw01: np.ndarray, cam_hw01: np.ndarray, alpha=0.45, cmap_name=\"jet\"):\n    \"\"\"\n    img_hw01: (H,W) in [0,1]\n    cam_hw01: (H,W) in [0,1]\n    return: (H,W,3) in [0,1] (overlayed RGB)\n    \"\"\"\n    # CAM -> 컬러(RGB)\n    heat = cm.get_cmap(cmap_name)(cam_hw01)[..., :3]  # (H,W,3)\n\n    # grayscale -> RGB\n    img_rgb = np.repeat(img_hw01[..., None], 3, axis=2)  # (H,W,3)\n\n    # overlay\n    out = (1 - alpha) * img_rgb + alpha * heat\n    out = np.clip(out, 0, 1)\n    return out\n\n@torch.no_grad()\ndef get_pred_conf(out: torch.Tensor, pred_idx: int):\n    \"\"\"\n    out: (1,num_classes) logits 또는 log-prob\n    return: softmax 확률(0~1)\n    \"\"\"\n    prob = torch.softmax(out, dim=-1)[0, pred_idx].item()\n    return prob\n\ndef show_gradcam_grid(\n    model,\n    gradcam: \"GradCAM\",\n    images,               # iterable of torch.Tensor each: (1,1,H,W) or (1,H,W)\n    labels=None,          # optional iterable of int\n    device=None,\n    rows=4,\n    cols=4,\n    alpha=0.45,\n    cmap_name=\"jet\",\n    class_idx=None,       # None이면 예측 클래스로 CAM\n    title_prefix=\"\",\n):\n    model.eval()\n    n = rows * cols\n\n    fig, axes = plt.subplots(rows, cols, figsize=(cols * 3.2, rows * 3.2))\n    axes = np.array(axes).reshape(-1)\n\n    for i in range(n):\n        ax = axes[i]\n        ax.axis(\"off\")\n\n        if i >= len(images):\n            continue\n\n        x = images[i]\n        if x.ndim == 3:  # (1,H,W) -> (1,1,H,W)\n            x = x.unsqueeze(1)\n        if device is not None:\n            x = x.to(device)\n\n        # CAM 생성\n        cam, pred_idx, score = gradcam.generate(x, class_idx=class_idx)\n\n        # 오버레이 생성\n        img_hw01 = to_numpy_img(x)           # (H,W)\n        cam_hw01 = cam.numpy()               # (H,W) already 0~1\n        overlay = overlay_cam_on_grayscale(img_hw01, cam_hw01, alpha=alpha, cmap_name=cmap_name)\n\n        ax.imshow(overlay)\n\n        # (옵션) 예측/정답 표시\n        gt = None if labels is None else int(labels[i])\n        # score는 logit/log-prob일 수 있으니 softmax 확률도 같이 보고 싶으면 아래처럼:\n        with torch.no_grad():\n            out = model(x)\n            conf = get_pred_conf(out, pred_idx)\n\n        if gt is None:\n            ax.set_title(f\"{title_prefix}pred={pred_idx} (p={conf:.2f})\", fontsize=10)\n        else:\n            ax.set_title(f\"{title_prefix}gt={gt} / pred={pred_idx} (p={conf:.2f})\", fontsize=10)\n\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = next(model.parameters()).device  # 모델 device\ngc = GradCAM(model, target_layer_idx=10)\n\n# 예: test_loader에서 16장만 확보\nimages = []\nlabels = []\nfor x, y in test_loader:\n    # x: (B,1,H,W)\n    for j in range(x.size(0)):\n        images.append(x[j:j+1])  # (1,1,H,W)\n        labels.append(int(y[j].item()))\n        if len(images) >= 16:\n            break\n    if len(images) >= 16:\n        break\n\nshow_gradcam_grid(model, gc, images, labels=labels, device=device, rows=4, cols=4, alpha=0.45)\ngc.remove_hooks()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 틀린이미지만 Grad-CAM","metadata":{}},{"cell_type":"code","source":"device = next(model.parameters()).device\ngc = GradCAM(model, target_layer_idx=10)\n\nwrong_imgs, wrong_lbls = [], []\nmodel.eval()\n\nfor x, y in test_loader:\n    x, y = x.to(device), y.to(device)\n    out = model(x)\n    pred = out.argmax(dim=1)\n    wrong_mask = (pred != y)\n\n    if wrong_mask.any():\n        idxs = wrong_mask.nonzero(as_tuple=False).squeeze(1).tolist()\n        for j in idxs:\n            wrong_imgs.append(x[j:j+1].detach().cpu())  # 나중에 show에서 device로 올림\n            wrong_lbls.append(int(y[j].item()))\n            if len(wrong_imgs) >= 16:\n                break\n    if len(wrong_imgs) >= 16:\n        break\n\nshow_gradcam_grid(model, gc, wrong_imgs, labels=wrong_lbls, device=device, rows=4, cols=4, alpha=0.45)\ngc.remove_hooks()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}